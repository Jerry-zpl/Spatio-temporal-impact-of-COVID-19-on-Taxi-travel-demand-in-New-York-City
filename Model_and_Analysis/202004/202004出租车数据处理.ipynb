{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data202004Y = pd.read_csv(\"E:\\\\Capstone Project\\\\Raw_Data\\\\Taxi\\\\202004\\\\yellow_tripdata_2020-04.csv\")\n",
    "\n",
    "data202004Y = data202004Y.drop(['VendorID','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag',\n",
    "                               'payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge',\n",
    "                               'total_amount','congestion_surcharge'],axis=1)\n",
    "\n",
    "data202004Y = data202004Y.rename(columns={'tpep_pickup_datetime':'pickup_datetime','tpep_dropoff_datetime':'dropoff_datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data202004G = pd.read_csv(\"E:\\\\Capstone Project\\\\Raw_Data\\\\Taxi\\\\202004\\\\green_tripdata_2020-04.csv\")\n",
    "\n",
    "data202004G = data202004G.drop(['VendorID','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag',\n",
    "                               'payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge',\n",
    "                               'total_amount','congestion_surcharge','ehail_fee','trip_type'],axis=1)\n",
    "\n",
    "data202004G = data202004G.rename(columns={'lpep_pickup_datetime':'pickup_datetime','lpep_dropoff_datetime':'dropoff_datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data202004 = pd.concat([data202004Y, data202004G], axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [pickup_datetime, dropoff_datetime, PULocationID, DOLocationID]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = data202004\n",
    "print(data[data.isnull().values==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "data[\"dropoff_datetime\"] = pd.to_datetime(data[\"dropoff_datetime\"])\n",
    "data = data[(data[\"pickup_datetime\"] >=pd.to_datetime('20200401')) & (data[\"pickup_datetime\"] <= pd.to_datetime('20200501'))]\n",
    "data = data[(data[\"dropoff_datetime\"] >=pd.to_datetime('20200401')) & (data[\"dropoff_datetime\"] <= pd.to_datetime('20200501'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup202004 = data[[\"pickup_datetime\",\"PULocationID\"]]\n",
    "dropoff202004 = data[[\"dropoff_datetime\",\"DOLocationID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PULocationID\n",
      "pickup_datetime              \n",
      "0                        3883\n",
      "1                        1640\n",
      "2                         991\n",
      "3                         880\n",
      "4                        1365\n",
      "5                        3495\n",
      "6                       11504\n",
      "7                       16115\n",
      "8                       17372\n",
      "9                       15988\n",
      "10                      15621\n",
      "11                      15829\n",
      "12                      16576\n",
      "13                      17266\n",
      "14                      18443\n",
      "15                      19561\n",
      "16                      19348\n",
      "17                      18900\n",
      "18                      16142\n",
      "19                      13189\n",
      "20                      10356\n",
      "21                       6851\n",
      "22                       5870\n",
      "23                       6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "pick1 = pickup202004\n",
    "pick1[\"pickup_datetime\"] = pd.to_datetime(pick1[\"pickup_datetime\"],format='%Y-%m-%d')\n",
    "pick1[\"pickup_datetime\"] = pick1[\"pickup_datetime\"].dt.hour\n",
    "print(pick1.groupby('pickup_datetime').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  DOLocationID\n",
      "dropoff_datetime              \n",
      "0                         4657\n",
      "1                         1927\n",
      "2                         1062\n",
      "3                          849\n",
      "4                         1193\n",
      "5                         2868\n",
      "6                        10141\n",
      "7                        15587\n",
      "8                        17340\n",
      "9                        16342\n",
      "10                       15621\n",
      "11                       15831\n",
      "12                       16507\n",
      "13                       17067\n",
      "14                       18084\n",
      "15                       19308\n",
      "16                       19456\n",
      "17                       19212\n",
      "18                       16688\n",
      "19                       13425\n",
      "20                       10850\n",
      "21                        7305\n",
      "22                        5872\n",
      "23                        6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "drop1 = dropoff202004\n",
    "drop1[\"dropoff_datetime\"] = pd.to_datetime(drop1[\"dropoff_datetime\"],format='%Y-%m-%d')\n",
    "drop1[\"dropoff_datetime\"] = drop1[\"dropoff_datetime\"].dt.hour\n",
    "print(drop1.groupby('dropoff_datetime').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\202004.csv',index=False,sep=',')\n",
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\202004.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pick1 = data[[\"pickup_datetime\",\"PULocationID\"]]\n",
    "pick1['pickup_datetime'] = pick1['pickup_datetime'].apply(lambda x:x[0:13])\n",
    "period1 = pick1.groupby(['pickup_datetime','PULocationID']).size()\n",
    "period2 = pick1.groupby(['PULocationID','pickup_datetime']).size()\n",
    "period1.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\pick202004period1.csv',index=True,sep=',')\n",
    "hours = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\pick202004period1.csv\")\n",
    "hours = hours.rename(columns={'0':'Freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(hours)):\n",
    "        if hours.iloc[j,0][11:13] == '00':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '01':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '02': \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '03':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '04':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '05':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '06':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '07':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '08': \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '09':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '10':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '11':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '12':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '13':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '14':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '15':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '16':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '17':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '18':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '19':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '20':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '21':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '22':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '23':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        else:\n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = hours.groupby(['pickup_datetime','PULocationID'])['Freq'].sum()\n",
    "periods.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004periods.csv',index=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004periods.csv\")\n",
    "taxi_area = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\ZonalAttribute.txt\",sep = ',')\n",
    "periods = periods.rename(columns={'pickup_datetime':'time','PULocationID':'LocationID'})\n",
    "area = taxi_area[['location_i','taxi_density','taxi_commercial','road_density']]\n",
    "area = area.rename(columns={'location_i':'LocationID','taxi_density':'pop_density','taxi_commercial':'commercial_density'})\n",
    "area['LocationID'] = pd.to_numeric(area['LocationID']).round(0).astype(int)\n",
    "attributes = pd.merge(periods, area, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "attributes = attributes.dropna()\n",
    "attributes.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004attributes.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209,\n",
       "            17210,\n",
       "            ...\n",
       "            17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373,\n",
       "            17374],\n",
       "           dtype='int64', length=174)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004attributes.csv\")\n",
    "\n",
    "data['periods'] = ''\n",
    "for j in range(len(data)):\n",
    "    data.loc[j,'periods'] = data.loc[j,'time'][11]\n",
    "\n",
    "for j in range(len(data)):\n",
    "    if data.loc[j,'periods'] == '0' or data.loc[j,'periods'] == '1':\n",
    "        data.drop([j],axis=0, inplace = True)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "data[data['time'] == '2020-04-22 2'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748,\n",
       "            14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757,\n",
       "            14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766,\n",
       "            14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775,\n",
       "            14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784,\n",
       "            14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793,\n",
       "            14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802,\n",
       "            14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811,\n",
       "            14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820,\n",
       "            14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829,\n",
       "            14830, 14831, 14832, 14833, 14834],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2020-04-18 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686,\n",
       "            19687,\n",
       "            ...\n",
       "            19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785,\n",
       "            19786],\n",
       "           dtype='int64', length=109)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2020-04-24 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = data.iloc[0:14835,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train1['time'])\n",
    "train1['time'] = df_date.transform(train1['time'])\n",
    "\n",
    "\n",
    "train2 = data.iloc[0:17201,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train2['time'])\n",
    "train2['time'] = df_date.transform(train2['time'])\n",
    "\n",
    "\n",
    "train3 = data.iloc[0:19787,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train3['time'])\n",
    "train3['time'] = df_date.transform(train3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = data.iloc[14835:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test1['time'])\n",
    "test1['time'] = df_date.transform(test1['time'])\n",
    "\n",
    "\n",
    "test2 = data.iloc[17201:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test2['time'])\n",
    "test2['time'] = df_date.transform(test2['time'])\n",
    "\n",
    "\n",
    "test3 = data.iloc[19787:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test3['time'])\n",
    "test3['time'] = df_date.transform(test3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all1 = data.iloc[:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(all1['time'])\n",
    "all1['time'] = df_date.transform(all1['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train1.csv',index=False,sep=',')\n",
    "train2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train2.csv',index=False,sep=',')\n",
    "train3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train3.csv',index=False,sep=',')\n",
    "test1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test1.csv',index=False,sep=',')\n",
    "test2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test2.csv',index=False,sep=',')\n",
    "test3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test3.csv',index=False,sep=',')\n",
    "all1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004all.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004all.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "final = pd.merge(data, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "final.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train1.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test1.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train1final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test1final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train2.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test2.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train2final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test2final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train3.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test3.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004train3final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick202004test3final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理dropoff数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\202004.csv\")\n",
    "drop1 = data[[\"dropoff_datetime\",\"DOLocationID\"]]\n",
    "drop1['dropoff_datetime'] = drop1['dropoff_datetime'].apply(lambda x:x[0:13])\n",
    "period3 = drop1.groupby(['dropoff_datetime','DOLocationID']).size()\n",
    "period4 = drop1.groupby(['DOLocationID','dropoff_datetime']).size()\n",
    "period3.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\drop202004period3.csv',index=True,sep=',')\n",
    "hours = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\drop202004period3.csv\")\n",
    "hours = hours.rename(columns={'0':'Freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(hours)):\n",
    "        if hours.iloc[j,0][11:13] == '00':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '01':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '02': \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '03':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '04':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '05':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '06':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '07':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '08': \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '09':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '10':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '11':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '12':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '13':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '14':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '15':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '16':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '17':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '18':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '19':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '20':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '21':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '22':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '23':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        else:\n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = hours.groupby(['dropoff_datetime','DOLocationID'])['Freq'].sum()\n",
    "periods.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004periods.csv',index=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004periods.csv\")\n",
    "taxi_area = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\ZonalAttribute.txt\",sep = ',')\n",
    "periods = periods.rename(columns={'dropoff_datetime':'time','DOLocationID':'LocationID'})\n",
    "area = taxi_area[['location_i','taxi_density','taxi_commercial','road_density']]\n",
    "area = area.rename(columns={'location_i':'LocationID','taxi_density':'pop_density','taxi_commercial':'commercial_density'})\n",
    "area['LocationID'] = pd.to_numeric(area['LocationID']).round(0).astype(int)\n",
    "attributes = pd.merge(periods, area, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "attributes = attributes.dropna()\n",
    "attributes.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004attributes.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([20901, 20902, 20903, 20904, 20905, 20906, 20907, 20908, 20909,\n",
       "            20910,\n",
       "            ...\n",
       "            21050, 21051, 21052, 21053, 21054, 21055, 21056, 21057, 21058,\n",
       "            21059],\n",
       "           dtype='int64', length=159)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004attributes.csv\")\n",
    "\n",
    "data['periods'] = ''\n",
    "for j in range(len(data)):\n",
    "    data.loc[j,'periods'] = data.loc[j,'time'][11]\n",
    "\n",
    "for j in range(len(data)):\n",
    "    if data.loc[j,'periods'] == '0' or data.loc[j,'periods'] == '1':\n",
    "        data.drop([j],axis=0, inplace = True)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "data[data['time'] == '2020-04-22 2'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853,\n",
       "            17854,\n",
       "            ...\n",
       "            17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991,\n",
       "            17992],\n",
       "           dtype='int64', length=148)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2020-04-18 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([23819, 23820, 23821, 23822, 23823, 23824, 23825, 23826, 23827,\n",
       "            23828,\n",
       "            ...\n",
       "            23976, 23977, 23978, 23979, 23980, 23981, 23982, 23983, 23984,\n",
       "            23985],\n",
       "           dtype='int64', length=167)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2020-04-24 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = data.iloc[0:17993,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train1['time'])\n",
    "train1['time'] = df_date.transform(train1['time'])\n",
    "\n",
    "\n",
    "train2 = data.iloc[0:20901,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train2['time'])\n",
    "train2['time'] = df_date.transform(train2['time'])\n",
    "\n",
    "\n",
    "train3 = data.iloc[0:23986,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train3['time'])\n",
    "train3['time'] = df_date.transform(train3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = data.iloc[17993:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test1['time'])\n",
    "test1['time'] = df_date.transform(test1['time'])\n",
    "\n",
    "\n",
    "test2 = data.iloc[20901:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test2['time'])\n",
    "test2['time'] = df_date.transform(test2['time'])\n",
    "\n",
    "\n",
    "test3 = data.iloc[23986:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test3['time'])\n",
    "test3['time'] = df_date.transform(test3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all1 = data.iloc[:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(all1['time'])\n",
    "all1['time'] = df_date.transform(all1['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train1.csv',index=False,sep=',')\n",
    "train2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train2.csv',index=False,sep=',')\n",
    "train3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train3.csv',index=False,sep=',')\n",
    "test1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test1.csv',index=False,sep=',')\n",
    "test2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test2.csv',index=False,sep=',')\n",
    "test3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test3.csv',index=False,sep=',')\n",
    "all1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004all.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004all.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "final = pd.merge(data, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "final.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train1.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test1.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train1final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test1final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train2.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test2.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train2final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test2final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train3.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test3.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004train3final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop202004test3final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
