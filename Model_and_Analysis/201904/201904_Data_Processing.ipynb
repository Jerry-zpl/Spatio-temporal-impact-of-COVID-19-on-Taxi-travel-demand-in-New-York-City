{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data201904Y = pd.read_csv(\"E:\\\\Capstone Project\\\\Raw_Data\\\\Taxi\\\\201904\\\\yellow_tripdata_2019-04.csv\")\n",
    "\n",
    "data201904Y = data201904Y.drop(['VendorID','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag',\n",
    "                               'payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge',\n",
    "                               'total_amount','congestion_surcharge'],axis=1)\n",
    "\n",
    "data201904Y = data201904Y.rename(columns={'tpep_pickup_datetime':'pickup_datetime','tpep_dropoff_datetime':'dropoff_datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data201904G = pd.read_csv(\"E:\\\\Capstone Project\\\\Raw_Data\\\\Taxi\\\\201904\\\\green_tripdata_2019-04.csv\")\n",
    "\n",
    "data201904G = data201904G.drop(['VendorID','passenger_count','trip_distance','RatecodeID','store_and_fwd_flag',\n",
    "                               'payment_type','fare_amount','extra','mta_tax','tip_amount','tolls_amount','improvement_surcharge',\n",
    "                               'total_amount','congestion_surcharge','ehail_fee','trip_type'],axis=1)\n",
    "\n",
    "data201904G = data201904G.rename(columns={'lpep_pickup_datetime':'pickup_datetime','lpep_dropoff_datetime':'dropoff_datetime'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data201904 = pd.concat([data201904Y, data201904G], axis=0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [pickup_datetime, dropoff_datetime, PULocationID, DOLocationID]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "data = data201904\n",
    "print(data[data.isnull().values==True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"pickup_datetime\"] = pd.to_datetime(data[\"pickup_datetime\"])\n",
    "data[\"dropoff_datetime\"] = pd.to_datetime(data[\"dropoff_datetime\"])\n",
    "data = data[(data[\"pickup_datetime\"] >=pd.to_datetime('20190401')) & (data[\"pickup_datetime\"] <= pd.to_datetime('20190501'))]\n",
    "data = data[(data[\"dropoff_datetime\"] >=pd.to_datetime('20190401')) & (data[\"dropoff_datetime\"] <= pd.to_datetime('20190501'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup201904 = data[[\"pickup_datetime\",\"PULocationID\"]]\n",
    "dropoff201904 = data[[\"dropoff_datetime\",\"DOLocationID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 PULocationID\n",
      "pickup_datetime              \n",
      "0                      221234\n",
      "1                      151544\n",
      "2                      104856\n",
      "3                       73467\n",
      "4                       58603\n",
      "5                       71519\n",
      "6                      164078\n",
      "7                      292809\n",
      "8                      364886\n",
      "9                      371015\n",
      "10                     374910\n",
      "11                     390996\n",
      "12                     408645\n",
      "13                     411262\n",
      "14                     436283\n",
      "15                     444253\n",
      "16                     424436\n",
      "17                     483173\n",
      "18                     535533\n",
      "19                     499019\n",
      "20                     463534\n",
      "21                     447247\n",
      "22                     424899\n",
      "23                     326730\n"
     ]
    }
   ],
   "source": [
    "pick1 = pickup201904\n",
    "pick1[\"pickup_datetime\"] = pd.to_datetime(pick1[\"pickup_datetime\"],format='%Y-%m-%d')\n",
    "pick1[\"pickup_datetime\"] = pick1[\"pickup_datetime\"].dt.hour\n",
    "print(pick1.groupby('pickup_datetime').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  DOLocationID\n",
      "dropoff_datetime              \n",
      "0                       246388\n",
      "1                       165057\n",
      "2                       113014\n",
      "3                        77336\n",
      "4                        61035\n",
      "5                        64187\n",
      "6                       142521\n",
      "7                       258934\n",
      "8                       346402\n",
      "9                       372532\n",
      "10                      371569\n",
      "11                      383549\n",
      "12                      407336\n",
      "13                      405802\n",
      "14                      425716\n",
      "15                      442116\n",
      "16                      427063\n",
      "17                      465974\n",
      "18                      542427\n",
      "19                      520492\n",
      "20                      468979\n",
      "21                      449053\n",
      "22                      433423\n",
      "23                      354026\n"
     ]
    }
   ],
   "source": [
    "drop1 = dropoff201904\n",
    "drop1[\"dropoff_datetime\"] = pd.to_datetime(drop1[\"dropoff_datetime\"],format='%Y-%m-%d')\n",
    "drop1[\"dropoff_datetime\"] = drop1[\"dropoff_datetime\"].dt.hour\n",
    "print(drop1.groupby('dropoff_datetime').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\201904.csv',index=False,sep=',')\n",
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\201904.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pick1 = data[[\"pickup_datetime\",\"PULocationID\"]]\n",
    "pick1['pickup_datetime'] = pick1['pickup_datetime'].apply(lambda x:x[0:13])\n",
    "period1 = pick1.groupby(['pickup_datetime','PULocationID']).size()\n",
    "period2 = pick1.groupby(['PULocationID','pickup_datetime']).size()\n",
    "period1.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\pick201904period1.csv',index=True,sep=',')\n",
    "hours = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\pick201904period1.csv\")\n",
    "hours = hours.rename(columns={'0':'Freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(hours)):\n",
    "        if hours.iloc[j,0][11:13] == '00':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '01':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '02': \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '03':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '04':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '05':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '06':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '07':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '08': \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '09':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '10':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '11':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '12':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '13':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '14':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '15':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '16':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '17':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '18':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '19':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '20':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '21':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '22':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '23':  \n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime'][0:11] + '7'\n",
    "        else:\n",
    "            hours.loc[j,'pickup_datetime'] = hours.loc[j,'pickup_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = hours.groupby(['pickup_datetime','PULocationID'])['Freq'].sum()\n",
    "periods.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904periods.csv',index=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904periods.csv\")\n",
    "taxi_area = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\ZonalAttribute.txt\",sep = ',')\n",
    "periods = periods.rename(columns={'pickup_datetime':'time','PULocationID':'LocationID'})\n",
    "area = taxi_area[['location_i','taxi_density','taxi_commercial','road_density']]\n",
    "area = area.rename(columns={'location_i':'LocationID','taxi_density':'pop_density','taxi_commercial':'commercial_density'})\n",
    "area['LocationID'] = pd.to_numeric(area['LocationID']).round(0).astype(int)\n",
    "attributes = pd.merge(periods, area, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "attributes = attributes.dropna()\n",
    "attributes.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904attributes.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([27128, 27129, 27130, 27131, 27132, 27133, 27134, 27135, 27136,\n",
       "            27137,\n",
       "            ...\n",
       "            27330, 27331, 27332, 27333, 27334, 27335, 27336, 27337, 27338,\n",
       "            27339],\n",
       "           dtype='int64', length=212)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904attributes.csv\")\n",
    "\n",
    "data['periods'] = ''\n",
    "for j in range(len(data)):\n",
    "    data.loc[j,'periods'] = data.loc[j,'time'][11]\n",
    "\n",
    "for j in range(len(data)):\n",
    "    if data.loc[j,'periods'] == '0' or data.loc[j,'periods'] == '1':\n",
    "        data.drop([j],axis=0, inplace = True)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "data[data['time'] == '2019-04-22 2'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([23238, 23239, 23240, 23241, 23242, 23243, 23244, 23245, 23246,\n",
       "            23247,\n",
       "            ...\n",
       "            23411, 23412, 23413, 23414, 23415, 23416, 23417, 23418, 23419,\n",
       "            23420],\n",
       "           dtype='int64', length=183)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2019-04-18 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([30675, 30676, 30677, 30678, 30679, 30680, 30681, 30682, 30683,\n",
       "            30684,\n",
       "            ...\n",
       "            30850, 30851, 30852, 30853, 30854, 30855, 30856, 30857, 30858,\n",
       "            30859],\n",
       "           dtype='int64', length=185)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2019-04-24 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = data.iloc[0:23421,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train1['time'])\n",
    "train1['time'] = df_date.transform(train1['time'])\n",
    "\n",
    "\n",
    "train2 = data.iloc[0:27128,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train2['time'])\n",
    "train2['time'] = df_date.transform(train2['time'])\n",
    "\n",
    "\n",
    "train3 = data.iloc[0:30860,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train3['time'])\n",
    "train3['time'] = df_date.transform(train3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = data.iloc[23421:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test1['time'])\n",
    "test1['time'] = df_date.transform(test1['time'])\n",
    "\n",
    "\n",
    "test2 = data.iloc[27128:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test2['time'])\n",
    "test2['time'] = df_date.transform(test2['time'])\n",
    "\n",
    "\n",
    "test3 = data.iloc[30860:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test3['time'])\n",
    "test3['time'] = df_date.transform(test3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all1 = data.iloc[:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(all1['time'])\n",
    "all1['time'] = df_date.transform(all1['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train1.csv',index=False,sep=',')\n",
    "train2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train2.csv',index=False,sep=',')\n",
    "train3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train3.csv',index=False,sep=',')\n",
    "test1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test1.csv',index=False,sep=',')\n",
    "test2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test2.csv',index=False,sep=',')\n",
    "test3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test3.csv',index=False,sep=',')\n",
    "all1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904all.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904all.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "final = pd.merge(data, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "final.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train1.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test1.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train1final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test1final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train2.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test2.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train2final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test2final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train3.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test3.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904train3final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\pick201904test3final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理dropoff数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\201904.csv\")\n",
    "drop1 = data[[\"dropoff_datetime\",\"DOLocationID\"]]\n",
    "drop1['dropoff_datetime'] = drop1['dropoff_datetime'].apply(lambda x:x[0:13])\n",
    "period3 = drop1.groupby(['dropoff_datetime','DOLocationID']).size()\n",
    "period4 = drop1.groupby(['DOLocationID','dropoff_datetime']).size()\n",
    "period3.to_csv(r'E:\\\\Capstone Project\\\\experiment\\\\drop201904period3.csv',index=True,sep=',')\n",
    "hours = pd.read_csv(\"E:\\\\Capstone Project\\\\experiment\\\\drop201904period3.csv\")\n",
    "hours = hours.rename(columns={'0':'Freq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(hours)):\n",
    "        if hours.iloc[j,0][11:13] == '00':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '01':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '02': \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '0'\n",
    "        elif hours.iloc[j,0][11:13] == '03':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '04':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '05':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '1'\n",
    "        elif hours.iloc[j,0][11:13] == '06':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '07':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '08': \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '2'\n",
    "        elif hours.iloc[j,0][11:13] == '09':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '10':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '11':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '3'\n",
    "        elif hours.iloc[j,0][11:13] == '12':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '13':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '14':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '4'\n",
    "        elif hours.iloc[j,0][11:13] == '15':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '16':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '17':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '5'\n",
    "        elif hours.iloc[j,0][11:13] == '18':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '19':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '20':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '6'\n",
    "        elif hours.iloc[j,0][11:13] == '21':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '22':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        elif hours.iloc[j,0][11:13] == '23':  \n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime'][0:11] + '7'\n",
    "        else:\n",
    "            hours.loc[j,'dropoff_datetime'] = hours.loc[j,'dropoff_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = hours.groupby(['dropoff_datetime','DOLocationID'])['Freq'].sum()\n",
    "periods.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904periods.csv',index=True,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904periods.csv\")\n",
    "taxi_area = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\ZonalAttribute.txt\",sep = ',')\n",
    "periods = periods.rename(columns={'dropoff_datetime':'time','DOLocationID':'LocationID'})\n",
    "area = taxi_area[['location_i','taxi_density','taxi_commercial','road_density']]\n",
    "area = area.rename(columns={'location_i':'LocationID','taxi_density':'pop_density','taxi_commercial':'commercial_density'})\n",
    "area['LocationID'] = pd.to_numeric(area['LocationID']).round(0).astype(int)\n",
    "attributes = pd.merge(periods, area, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "attributes = attributes.dropna()\n",
    "attributes.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904attributes.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([29927, 29928, 29929, 29930, 29931, 29932, 29933, 29934, 29935,\n",
       "            29936,\n",
       "            ...\n",
       "            30137, 30138, 30139, 30140, 30141, 30142, 30143, 30144, 30145,\n",
       "            30146],\n",
       "           dtype='int64', length=220)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904attributes.csv\")\n",
    "\n",
    "data['periods'] = ''\n",
    "for j in range(len(data)):\n",
    "    data.loc[j,'periods'] = data.loc[j,'time'][11]\n",
    "\n",
    "for j in range(len(data)):\n",
    "    if data.loc[j,'periods'] == '0' or data.loc[j,'periods'] == '1':\n",
    "        data.drop([j],axis=0, inplace = True)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "data[data['time'] == '2019-04-22 2'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([25460, 25461, 25462, 25463, 25464, 25465, 25466, 25467, 25468,\n",
       "            25469,\n",
       "            ...\n",
       "            25695, 25696, 25697, 25698, 25699, 25700, 25701, 25702, 25703,\n",
       "            25704],\n",
       "           dtype='int64', length=245)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2019-04-18 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([33928, 33929, 33930, 33931, 33932, 33933, 33934, 33935, 33936,\n",
       "            33937,\n",
       "            ...\n",
       "            34162, 34163, 34164, 34165, 34166, 34167, 34168, 34169, 34170,\n",
       "            34171],\n",
       "           dtype='int64', length=244)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['time'] == '2019-04-24 7'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = data.iloc[0:25705,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train1['time'])\n",
    "train1['time'] = df_date.transform(train1['time'])\n",
    "\n",
    "\n",
    "train2 = data.iloc[0:29927,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train2['time'])\n",
    "train2['time'] = df_date.transform(train2['time'])\n",
    "\n",
    "\n",
    "train3 = data.iloc[0:34172,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(train3['time'])\n",
    "train3['time'] = df_date.transform(train3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = data.iloc[25705:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test1['time'])\n",
    "test1['time'] = df_date.transform(test1['time'])\n",
    "\n",
    "\n",
    "test2 = data.iloc[29927:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test2['time'])\n",
    "test2['time'] = df_date.transform(test2['time'])\n",
    "\n",
    "\n",
    "test3 = data.iloc[34172:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(test3['time'])\n",
    "test3['time'] = df_date.transform(test3['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all1 = data.iloc[:,0:6]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_date = le.fit(all1['time'])\n",
    "all1['time'] = df_date.transform(all1['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train1.csv',index=False,sep=',')\n",
    "train2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train2.csv',index=False,sep=',')\n",
    "train3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train3.csv',index=False,sep=',')\n",
    "test1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test1.csv',index=False,sep=',')\n",
    "test2.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test2.csv',index=False,sep=',')\n",
    "test3.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test3.csv',index=False,sep=',')\n",
    "all1.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904all.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904all.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "final = pd.merge(data, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "final.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train1.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test1.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train1final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test1final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train2.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test2.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train2final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test2final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train3.csv\")\n",
    "test = pd.read_csv(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test3.csv\")\n",
    "coor = pd.read_table(\"E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\Coordinate.txt\",sep = ',')\n",
    "coor = coor[['location_i','X','Y']]\n",
    "coor = coor.rename(columns={'location_i':'LocationID'})\n",
    "coor['LocationID'] = pd.to_numeric(coor['LocationID']).round(0).astype(int)\n",
    "trainfinal = pd.merge(train, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "testfinal = pd.merge(test, coor, left_on='LocationID', right_on='LocationID' , how='left')\n",
    "trainfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904train3final.csv',index=False,sep=',')\n",
    "testfinal.to_csv(r'E:\\\\Capstone Project\\\\Data\\\\Taxi_area\\\\drop201904test3final.csv',index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
